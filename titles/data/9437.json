{
    "url": "https://api.github.com/repos/spack/spack/issues/9437",
    "repository_url": "https://api.github.com/repos/spack/spack",
    "labels_url": "https://api.github.com/repos/spack/spack/issues/9437/labels{/name}",
    "comments_url": "https://api.github.com/repos/spack/spack/issues/9437/comments",
    "events_url": "https://api.github.com/repos/spack/spack/issues/9437/events",
    "html_url": "https://github.com/spack/spack/issues/9437",
    "id": 366865898,
    "node_id": "MDU6SXNzdWUzNjY4NjU4OTg=",
    "number": 9437,
    "title": "Installation issue: spark+hadoop",
    "user": {
        "login": "mpbelhorn",
        "id": 1690817,
        "node_id": "MDQ6VXNlcjE2OTA4MTc=",
        "avatar_url": "https://avatars.githubusercontent.com/u/1690817?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/mpbelhorn",
        "html_url": "https://github.com/mpbelhorn",
        "followers_url": "https://api.github.com/users/mpbelhorn/followers",
        "following_url": "https://api.github.com/users/mpbelhorn/following{/other_user}",
        "gists_url": "https://api.github.com/users/mpbelhorn/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/mpbelhorn/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/mpbelhorn/subscriptions",
        "organizations_url": "https://api.github.com/users/mpbelhorn/orgs",
        "repos_url": "https://api.github.com/users/mpbelhorn/repos",
        "events_url": "https://api.github.com/users/mpbelhorn/events{/privacy}",
        "received_events_url": "https://api.github.com/users/mpbelhorn/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2018-10-04T16:12:55Z",
    "updated_at": "2018-10-18T19:45:12Z",
    "closed_at": "2018-10-18T19:45:12Z",
    "author_association": "CONTRIBUTOR",
    "active_lock_reason": null,
    "body": "### The problem\r\n\r\nWhen spark is built with hadoop, the comand `hadoop classpath` is run to setup the environment:\r\n\r\nspark/package.py\r\n```python\r\n    @when('+hadoop')\r\n    def setup_environment(self, spack_env, run_env):\r\n        hadoop = self.spec['hadoop'].command\r\n        hadoop_classpath = hadoop('classpath', output=str)\r\n\r\n        # Remove whitespaces, as they can compromise syntax in\r\n        # module files\r\n        hadoop_classpath = re.sub('[\\s+]', '', hadoop_classpath)\r\n\r\n        run_env.set('SPARK_DIST_CLASSPATH', hadoop_classpath)\r\n```\r\n\r\nHowever, this command needs the `JAVA_HOME` variable set, which does not happen either \r\n\r\n- before `spark.setup_environment` is called xor\r\n- in the `hadoop` subprocess\r\n\r\ncausing the build to fail when a java jdk is not already in the environment (and may not be the same JDK that the rest of hadoop and spark will be built with).\r\n\r\n### Possible solution\r\n\r\nPut the `JAVA_HOME` into the `hadoop classpath` subprocess environment:\r\n\r\n```diff\r\n diff --git a/var/spack/repos/builtin/packages/spark/package.py b/var/spack/repos/builtin/packages/spark/package.py\r\nindex 9dce9b81f..aa50245c8 100644\r\n--- a/var/spack/repos/builtin/packages/spark/package.py\r\n+++ b/var/spack/repos/builtin/packages/spark/package.py\r\n@@ -68,6 +68,7 @@ class Spark(Package):\r\n     @when('+hadoop')\r\n     def setup_environment(self, spack_env, run_env):\r\n         hadoop = self.spec['hadoop'].command\r\n+        hadoop.add_default_env('JAVA_HOME', self.spec['java'].home)\r\n         hadoop_classpath = hadoop('classpath', output=str)\r\n \r\n         # Remove whitespaces, as they can compromise syntax in\r\n\r\n```\r\n\r\n### Steps to reproduce the issue\r\n\r\n```console\r\n$ ./bin/spack spec -lINt 'spark@2.3.0%gcc@6.2.0+hadoop'                                                                                                                                                                     \r\nInput spec                                                                                                                                                                                                                                     \r\n--------------------------------                                                                                                                                                                                                               \r\n     [    ]  spark@2.3.0%gcc@6.2.0+hadoop                                                                                                                                                                                                      \r\n                                                                                                                                                                                                                                               \r\nConcretized                                                                                                                                                                                                                                    \r\n--------------------------------                                                                                                                                                                                                               \r\n     pxqqqhv  [    ]  builtin.spark@2.3.0%gcc@6.2.0+hadoop arch=linux-rhel7-x86_64                                                                                                                                                             \r\n[+]  572r7i7  [b r ]      ^builtin.hadoop@3.1.1%gcc@6.2.0 arch=linux-rhel7-x86_64                                                                                                                                                              \r\n[+]  vrwoesl  [b r ]          ^builtin.jdk@10.0.2_13%gcc@6.2.0 arch=linux-rhel7-x86_64                                                                                                                                                         \r\n                                                                                                                                                                                                                                               \r\n$ ./bin/spack spec -lINt 'spark@2.3.0%gcc@6.2.0+hadoop ^jdk@10.0.2_13'                                                                                                                                                      \r\nInput spec                                                                                                                                                                                                                                     \r\n--------------------------------                                                                                                                                                                                                               \r\n     [    ]  spark@2.3.0%gcc@6.2.0+hadoop                                                                                                                                                                                                      \r\n     [    ]      ^jdk@10.0.2_13                                                                                                                                                                                                                \r\n                                                                                                                                                                                                                                               \r\nConcretized                                                                                                                                                                                                                                    \r\n--------------------------------                                                                                                                                                                                                               \r\n     pxqqqhv  [    ]  builtin.spark@2.3.0%gcc@6.2.0+hadoop arch=linux-rhel7-x86_64                                                                                                                                                             \r\n[+]  572r7i7  [b r ]      ^builtin.hadoop@3.1.1%gcc@6.2.0 arch=linux-rhel7-x86_64                                                                                                                                                              \r\n[+]  vrwoesl  [b r ]          ^builtin.jdk@10.0.2_13%gcc@6.2.0 arch=linux-rhel7-x86_64                                                                                                                                                         \r\n                                                                                                                                                                                                                                               \r\n$ ./bin/spack install -v 'spark@2.3.0%gcc@6.2.0+hadoop ^jdk@10.0.2_13'                                                                                                                                                      \r\n==> jdk is already installed in /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/jdk-10.0.2_13-vrwoesl2avpqbyben7qhedo7jwflaxit                                                                 \r\n==> hadoop is already installed in /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/hadoop-3.1.1-572r7i7vahxzfjuowa26xpxicuv6xytq                                                               \r\n==> Installing spark                                                                                                                                                                                                                           \r\nERROR: JAVA_HOME is not set and could not be found.                                                                                                                                                                                            \r\n==> Error: ProcessError: Command exited with status 1:                                                                                                                                                                                         \r\n    '/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/hadoop-3.1.1-572r7i7vahxzfjuowa26xpxicuv6xytq/bin/hadoop' 'classpath'\r\n$ ./bin/spack env 'spark@2.3.0%gcc@6.2.0+hadoop ^jdk@10.0.2_13'                          \r\nERROR: JAVA_HOME is not set and could not be found.\r\n==> Error: Command exited with status 1:                                                    \r\n'/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/hadoop-3.1.1-572r7i7vahxzfjuowa26xpxicuv6xytq/bin/hadoop' 'classpath'\r\n```\r\n\r\n### Platform and user environment\r\n\r\nPlease report your OS here:\r\n```commandline\r\n$ uname -a                                                                                                                                                                                                                 \r\nLinux rhea-login8g 3.10.0-862.14.4.el7.x86_64 #1 SMP Fri Sep 21 09:07:21 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux \r\n$ lsb_release -d                                                                                                                                                                                                           \r\nDescription:    Red Hat Enterprise Linux Server release 7.5 (Maipo)\r\n``` \r\n\r\n### Additional information\r\n\r\nError with stack trace:\r\n```console\r\n$ ./bin/spack -d env 'spark@2.3.0%gcc@6.2.0+hadoop ^jdk@10.0.2_13'                                                                    \r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/defaults/modules.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/defaults/linux/modules.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/modules.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/defaults/config.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/config.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/defaults/repos.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/repos.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/defaults/packages.yaml\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/packages.yaml\r\n==> READ LOCK: /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/mcache/providers/.builtin-index.yaml.lock[0:0] [Acquiring]\r\n==> READ LOCK: /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/mcache/providers/.builtin-index.yaml.lock[0:0] [Acquired]\r\n==> READ LOCK: /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/mcache/providers/.builtin-index.yaml.lock[0:0] [Released]\r\n==> READ LOCK: /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/mcache/providers/.olcf-index.yaml.lock[0:0] [Acquiring]\r\n==> READ LOCK: /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/mcache/providers/.olcf-index.yaml.lock[0:0] [Acquired]\r\n==> READ LOCK: /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/mcache/providers/.olcf-index.yaml.lock[0:0] [Released]\r\n==> Reading config file /autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/etc/spack/compilers.yaml\r\n==> Reading config file /ccs/home/uamntr/.spack/linux/compilers.yaml\r\n==> DATABASE LOCK TIMEOUT: 120s\r\n==> PACKAGE LOCK TIMEOUT: No timeout\r\n==> '/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/hadoop-3.1.1-572r7i7vahxzfjuowa26xpxicuv6xytq/bin/hadoop' 'classpath'\r\nERROR: JAVA_HOME is not set and could not be found.\r\n==> Error: Command exited with status 1:\r\n'/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/hadoop-3.1.1-572r7i7vahxzfjuowa26xpxicuv6xytq/bin/hadoop' 'classpath'\r\nTraceback (most recent call last):\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/lib/spack/spack/main.py\", line 653, in main\r\n    return _invoke_command(command, parser, args, unknown)\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/lib/spack/spack/main.py\", line 432, in _invoke_command\r\n    return_val = command(parser, args)\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/lib/spack/spack/cmd/env.py\", line 69, in env\r\n    build_env.setup_package(spec.package, args.dirty)\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/lib/spack/spack/build_environment.py\", line 666, in setup_package\r\n    pkg.setup_environment(spack_env, run_env)\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/lib/spack/spack/multimethod.py\", line 127, in __call__\r\n    return method(package_self, *args, **kwargs)\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/var/spack/repos/builtin/packages/spark/package.py\", line 72, in setup_environment\r\n\r\n  File \"/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/lib/spack/spack/util/executable.py\", line 208, in __call__\r\n    proc.returncode, long_msg)\r\nspack.util.executable.ProcessError: Command exited with status 1:\r\n    '/autofs/nccs-svm1_sw/.staging/rhea/.swci/0-core/opt/spack/20180914/linux-rhel7-x86_64/gcc-6.2.0/hadoop-3.1.1-572r7i7vahxzfjuowa26xpxicuv6xytq/bin/hadoop' 'classpath'\r\n```\r\n\r\n",
    "performed_via_github_app": null
}