{
    "url": "https://api.github.com/repos/spack/spack/issues/17227",
    "repository_url": "https://api.github.com/repos/spack/spack",
    "labels_url": "https://api.github.com/repos/spack/spack/issues/17227/labels{/name}",
    "comments_url": "https://api.github.com/repos/spack/spack/issues/17227/comments",
    "events_url": "https://api.github.com/repos/spack/spack/issues/17227/events",
    "html_url": "https://github.com/spack/spack/issues/17227",
    "id": 644746025,
    "node_id": "MDU6SXNzdWU2NDQ3NDYwMjU=",
    "number": 17227,
    "title": "Spack concretizer does not choose correct variant of external package.",
    "user": {
        "login": "roguephysicist",
        "id": 2207402,
        "node_id": "MDQ6VXNlcjIyMDc0MDI=",
        "avatar_url": "https://avatars.githubusercontent.com/u/2207402?v=4",
        "gravatar_id": "",
        "url": "https://api.github.com/users/roguephysicist",
        "html_url": "https://github.com/roguephysicist",
        "followers_url": "https://api.github.com/users/roguephysicist/followers",
        "following_url": "https://api.github.com/users/roguephysicist/following{/other_user}",
        "gists_url": "https://api.github.com/users/roguephysicist/gists{/gist_id}",
        "starred_url": "https://api.github.com/users/roguephysicist/starred{/owner}{/repo}",
        "subscriptions_url": "https://api.github.com/users/roguephysicist/subscriptions",
        "organizations_url": "https://api.github.com/users/roguephysicist/orgs",
        "repos_url": "https://api.github.com/users/roguephysicist/repos",
        "events_url": "https://api.github.com/users/roguephysicist/events{/privacy}",
        "received_events_url": "https://api.github.com/users/roguephysicist/received_events",
        "type": "User",
        "site_admin": false
    },
    "labels": [
        {
            "id": 73908754,
            "node_id": "MDU6TGFiZWw3MzkwODc1NA==",
            "url": "https://api.github.com/repos/spack/spack/labels/bug",
            "name": "bug",
            "color": "fc2929",
            "default": true,
            "description": null
        },
        {
            "id": 1433532775,
            "node_id": "MDU6TGFiZWwxNDMzNTMyNzc1",
            "url": "https://api.github.com/repos/spack/spack/labels/triage",
            "name": "triage",
            "color": "ed9793",
            "default": false,
            "description": "The issue needs to be prioritized"
        }
    ],
    "state": "closed",
    "locked": false,
    "assignee": null,
    "assignees": [],
    "milestone": null,
    "comments": 0,
    "created_at": "2020-06-24T16:23:00Z",
    "updated_at": "2020-12-09T19:46:18Z",
    "closed_at": "2020-12-09T19:46:17Z",
    "author_association": "NONE",
    "active_lock_reason": null,
    "body": "Spack will avoid concretizing with a specific variant of an external package. In this particular example, I want to build Quantum Espresso with Open MPI 4.0.3 compiled with the GCC 10.1.0.\r\n\r\nHere is an excerpt from my `packages.yaml`:\r\n\r\n```yaml\r\npackages:\r\n  all:\r\n    compiler: [gcc, intel]\r\n  openmpi:\r\n    modules:\r\n      openmpi@4.0.3%intel@18.0.5:     mpi/openmpi/4.0.3/intel/2018\r\n      openmpi@4.0.3%intel@19.1.0.166: mpi/openmpi/4.0.3/intel/2020\r\n      openmpi@4.0.3%gcc@6.5.0:        mpi/openmpi/4.0.3/gcc/6.5.0\r\n      openmpi@4.0.3%gcc@8.4.0:        mpi/openmpi/4.0.3/gcc/8.4.0\r\n      openmpi@4.0.3%gcc@9.3.0:        mpi/openmpi/4.0.3/gcc/9.3.0\r\n      openmpi@4.0.3%gcc@10.1.0:       mpi/openmpi/4.0.3/gcc/10.1.0\r\n    buildable: true\r\n```\r\n\r\n### Steps to reproduce the issue\r\n\r\nSpack concretizes correctly for GCC 6.5.0, 8.4.0, and 9.3.0, but when requesting GCC 10.1.0, it selects `openmpi@4.0.3%gcc@9.3.0`. See the following examples:\r\n\r\n```console\r\n$ spack spec -I quantum-espresso %gcc@6.5.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@6.5.0\r\nConcretized\r\n--------------------------------\r\n -   quantum-espresso@6.5%gcc@6.5.0 arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@6.5.0 arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@6.5.0 arch=linux-rhel7-haswell\r\n\r\n$ spack spec -I quantum-espresso %gcc@9.3.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@9.3.0\r\nConcretized\r\n--------------------------------\r\n -   quantum-espresso@6.5%gcc@9.3.0 arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@9.3.0 arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell\r\n\r\n$ spack spec -I quantum-espresso %gcc@10.1.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@10.1.0\r\nConcretized\r\n--------------------------------\r\n -   quantum-espresso@6.5%gcc@10.1.0 arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@10.1.0 arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell\r\n```\r\n\r\n### Error Message\r\n\r\nPer a suggestion from @becker33, I added\r\n\r\n```python\r\nprint(candidates)\r\nprint(spec)\r\n```\r\n\r\nto line 133 in `$SPACK/lib/spack/spack/concretize.py`. This produces the following for two attempts:\r\n\r\n```console\r\n$ spack spec -I quantum-espresso %gcc@6.5.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@6.5.0\r\nConcretized\r\n--------------------------------\r\n[quantum-espresso%gcc@6.5.0 ^blas ^fftw-api@3 ^lapack]\r\nquantum-espresso%gcc@6.5.0 ^blas ^fftw-api@3 ^lapack\r\n[intel-mkl@2020.0.166, intel-mkl@2018.5.274, intel-mkl, openblas, amdblis+blas, amdblis+cblas, atlas, blis+blas, blis+cblas, cray-libsci, essl, intel-parallel-studio+mkl, netlib-lapack~external-blas, netlib-xblas+plain_blas, veclibfort]\r\nblas\r\n[quantum-espresso%gcc@6.5.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@6.5.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166]\r\nquantum-espresso%gcc@6.5.0 ^intel-mkl@2020.0.166\r\n[quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi]\r\nquantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi\r\n[openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, intel-mpi@2020.0.166, intel-mpi@2018.5.274, openmpi@2.0.0:, openmpi@1.7.5:, openmpi@1.6.5, openmpi, mpich@3:, mpich@1:, mpich, mvapich2@2.3:, mvapich2@2.1:, mvapich2, fujitsu-mpi, intel-parallel-studio+mpi, mpilander, mpt@3:, mpt@1:, mpt, spectrum-mpi]\r\nmpi\r\n[quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0 arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0 arch=linux-rhel7-haswell\r\n[quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@6.5.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n -   quantum-espresso@6.5%gcc@6.5.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none patches=b1aa3179ee1c069964fb9c21f3b832aebeae54947ce8d3cc1a74e7b154c3c10f arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@6.5.0~ilp64+shared threads=none arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@6.5.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n\r\n$ spack spec -I quantum-espresso %gcc@10.1.0\r\nInput spec\r\n--------------------------------\r\n -   quantum-espresso%gcc@10.1.0\r\nConcretized\r\n--------------------------------\r\n[quantum-espresso%gcc@10.1.0 ^blas ^fftw-api@3 ^lapack]\r\nquantum-espresso%gcc@10.1.0 ^blas ^fftw-api@3 ^lapack\r\n[intel-mkl@2020.0.166, intel-mkl@2018.5.274, intel-mkl, openblas, amdblis+blas, amdblis+cblas, atlas, blis+blas, blis+cblas, cray-libsci, essl, intel-parallel-studio+mkl, netlib-lapack~external-blas, netlib-xblas+plain_blas, veclibfort]\r\nblas\r\n[quantum-espresso%gcc@10.1.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@10.1.0 ^fftw-api@3 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166 ^lapack]\r\nquantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166 ^lapack\r\n[quantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166]\r\nquantum-espresso%gcc@10.1.0 ^intel-mkl@2020.0.166\r\n[quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi]\r\nquantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^mpi\r\n[openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@6.5.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@8.4.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@9.3.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%gcc@10.1.0, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@18.0.5, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, openmpi@4.0.3%intel@19.1.0.166, intel-mpi@2020.0.166, intel-mpi@2018.5.274, openmpi@2.0.0:, openmpi@1.7.5:, openmpi@1.6.5, openmpi, mpich@3:, mpich@1:, mpich, mvapich2@2.3:, mvapich2@2.1:, mvapich2, fujitsu-mpi, intel-parallel-studio+mpi, mpilander, mpt@3:, mpt@1:, mpt, spectrum-mpi]\r\nmpi\r\n[quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0 arch=linux-rhel7-haswell\r\n[quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell]\r\nquantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none arch=linux-rhel7-haswell ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell ^openmpi@4.0.3%gcc@9.3.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n -   quantum-espresso@6.5%gcc@10.1.0~elpa~epw+mpi~openmp+patch~qmcpack+scalapack hdf5=none patches=b1aa3179ee1c069964fb9c21f3b832aebeae54947ce8d3cc1a74e7b154c3c10f arch=linux-rhel7-haswell\r\n -       ^intel-mkl@2020.0.166%gcc@10.1.0~ilp64+shared threads=none arch=linux-rhel7-haswell\r\n -       ^openmpi@4.0.3%gcc@9.3.0~atomics~cuda~cxx~cxx_exceptions+gpfs~java~legacylaunchers~memchecker~pmi~sqlite3+static~thread_multiple+vt+wrapper-rpath fabrics=none schedulers=none arch=linux-rhel7-haswell\r\n```\r\n\r\n### Information on your system\r\n\r\n* **Spack:** 0.14.2-1529-ec58f28\r\n* **Python:** 3.6.8\r\n* **Platform:** linux-rhel7-haswell\r\n\r\nSee the excerpt from `packages.yaml` above.\r\n\r\n### Additional information\r\n\r\n- [x] I have run `spack debug report` and reported the version of Spack/Python/Platform\r\n- [x] I have searched the issues of this repo and believe this is not a duplicate\r\n- [ ] I have run the failing commands in debug mode and reported the output",
    "performed_via_github_app": null
}